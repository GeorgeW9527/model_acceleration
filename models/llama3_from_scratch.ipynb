{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama3.2 from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "首先从huggingface下载Llama-3.2-3B-Instruct，具体步骤是：\n",
    "1. Pip安装huggingface-cli\n",
    "`pip install -U huggingface_hub`\n",
    "2. 执行`huggingface-cli login`，输入token(token来自于自己的注册)\n",
    "3. 然后用transformer接口下载模型，默认路径：~/.cache\n",
    "[参考](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/HOME/scz0101/.conda/envs/model_acc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0\n"
     ]
    }
   ],
   "source": [
    "# 读取并查看模型参数\n",
    "with open(\"/HOME/scz0101/run/model_acceleration/models/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "# print(config)\n",
    "print(config['rope_scaling']['factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"model.embed_tokens.weight\",\n",
      "    \"model.layers.0.input_layernorm.weight\",\n",
      "    \"model.layers.0.mlp.down_proj.weight\",\n",
      "    \"model.layers.0.mlp.gate_proj.weight\",\n",
      "    \"model.layers.0.mlp.up_proj.weight\",\n",
      "    \"model.layers.0.post_attention_layernorm.weight\",\n",
      "    \"model.layers.0.self_attn.k_proj.weight\",\n",
      "    \"model.layers.0.self_attn.o_proj.weight\",\n",
      "    \"model.layers.0.self_attn.q_proj.weight\",\n",
      "    \"model.layers.0.self_attn.v_proj.weight\",\n",
      "    \"model.layers.1.input_layernorm.weight\",\n",
      "    \"model.layers.1.mlp.down_proj.weight\",\n",
      "    \"model.layers.1.mlp.gate_proj.weight\",\n",
      "    \"model.layers.1.mlp.up_proj.weight\",\n",
      "    \"model.layers.1.post_attention_layernorm.weight\",\n",
      "    \"model.layers.1.self_attn.k_proj.weight\",\n",
      "    \"model.layers.1.self_attn.o_proj.weight\",\n",
      "    \"model.layers.1.self_attn.q_proj.weight\",\n",
      "    \"model.layers.1.self_attn.v_proj.weight\",\n",
      "    \"model.layers.10.input_layernorm.weight\",\n",
      "    \"model.layers.10.mlp.down_proj.weight\",\n",
      "    \"model.layers.10.mlp.gate_proj.weight\",\n",
      "    \"model.layers.10.mlp.up_proj.weight\",\n",
      "    \"model.layers.10.post_attention_layernorm.weight\",\n",
      "    \"model.layers.10.self_attn.k_proj.weight\",\n",
      "    \"model.layers.10.self_attn.o_proj.weight\",\n",
      "    \"model.layers.10.self_attn.q_proj.weight\",\n",
      "    \"model.layers.10.self_attn.v_proj.weight\",\n",
      "    \"model.layers.11.input_layernorm.weight\",\n",
      "    \"model.layers.11.mlp.down_proj.weight\",\n",
      "    \"model.layers.11.mlp.gate_proj.weight\",\n",
      "    \"model.layers.11.mlp.up_proj.weight\",\n",
      "    \"model.layers.11.post_attention_layernorm.weight\",\n",
      "    \"model.layers.11.self_attn.k_proj.weight\",\n",
      "    \"model.layers.11.self_attn.o_proj.weight\",\n",
      "    \"model.layers.11.self_attn.q_proj.weight\",\n",
      "    \"model.layers.11.self_attn.v_proj.weight\",\n",
      "    \"model.layers.12.input_layernorm.weight\",\n",
      "    \"model.layers.12.mlp.down_proj.weight\",\n",
      "    \"model.layers.12.mlp.gate_proj.weight\",\n",
      "    \"model.layers.12.mlp.up_proj.weight\",\n",
      "    \"model.layers.12.post_attention_layernorm.weight\",\n",
      "    \"model.layers.12.self_attn.k_proj.weight\",\n",
      "    \"model.layers.12.self_attn.o_proj.weight\",\n",
      "    \"model.layers.12.self_attn.q_proj.weight\",\n",
      "    \"model.layers.12.self_attn.v_proj.weight\",\n",
      "    \"model.layers.13.input_layernorm.weight\",\n",
      "    \"model.layers.13.mlp.down_proj.weight\",\n",
      "    \"model.layers.13.mlp.gate_proj.weight\",\n",
      "    \"model.layers.13.mlp.up_proj.weight\",\n",
      "    \"model.layers.13.post_attention_layernorm.weight\",\n",
      "    \"model.layers.13.self_attn.k_proj.weight\",\n",
      "    \"model.layers.13.self_attn.o_proj.weight\",\n",
      "    \"model.layers.13.self_attn.q_proj.weight\",\n",
      "    \"model.layers.13.self_attn.v_proj.weight\",\n",
      "    \"model.layers.14.input_layernorm.weight\",\n",
      "    \"model.layers.14.mlp.down_proj.weight\",\n",
      "    \"model.layers.14.mlp.gate_proj.weight\",\n",
      "    \"model.layers.14.mlp.up_proj.weight\",\n",
      "    \"model.layers.14.post_attention_layernorm.weight\",\n",
      "    \"model.layers.14.self_attn.k_proj.weight\",\n",
      "    \"model.layers.14.self_attn.o_proj.weight\",\n",
      "    \"model.layers.14.self_attn.q_proj.weight\",\n",
      "    \"model.layers.14.self_attn.v_proj.weight\",\n",
      "    \"model.layers.15.input_layernorm.weight\",\n",
      "    \"model.layers.15.mlp.down_proj.weight\",\n",
      "    \"model.layers.15.mlp.gate_proj.weight\",\n",
      "    \"model.layers.15.mlp.up_proj.weight\",\n",
      "    \"model.layers.15.post_attention_layernorm.weight\",\n",
      "    \"model.layers.15.self_attn.k_proj.weight\",\n",
      "    \"model.layers.15.self_attn.o_proj.weight\",\n",
      "    \"model.layers.15.self_attn.q_proj.weight\",\n",
      "    \"model.layers.15.self_attn.v_proj.weight\",\n",
      "    \"model.layers.16.input_layernorm.weight\",\n",
      "    \"model.layers.16.mlp.down_proj.weight\",\n",
      "    \"model.layers.16.mlp.gate_proj.weight\",\n",
      "    \"model.layers.16.mlp.up_proj.weight\",\n",
      "    \"model.layers.16.post_attention_layernorm.weight\",\n",
      "    \"model.layers.16.self_attn.k_proj.weight\",\n",
      "    \"model.layers.16.self_attn.o_proj.weight\",\n",
      "    \"model.layers.16.self_attn.q_proj.weight\",\n",
      "    \"model.layers.16.self_attn.v_proj.weight\",\n",
      "    \"model.layers.17.input_layernorm.weight\",\n",
      "    \"model.layers.17.mlp.down_proj.weight\",\n",
      "    \"model.layers.17.mlp.gate_proj.weight\",\n",
      "    \"model.layers.17.mlp.up_proj.weight\",\n",
      "    \"model.layers.17.post_attention_layernorm.weight\",\n",
      "    \"model.layers.17.self_attn.k_proj.weight\",\n",
      "    \"model.layers.17.self_attn.o_proj.weight\",\n",
      "    \"model.layers.17.self_attn.q_proj.weight\",\n",
      "    \"model.layers.17.self_attn.v_proj.weight\",\n",
      "    \"model.layers.18.input_layernorm.weight\",\n",
      "    \"model.layers.18.mlp.down_proj.weight\",\n",
      "    \"model.layers.18.mlp.gate_proj.weight\",\n",
      "    \"model.layers.18.mlp.up_proj.weight\",\n",
      "    \"model.layers.18.post_attention_layernorm.weight\",\n",
      "    \"model.layers.18.self_attn.k_proj.weight\",\n",
      "    \"model.layers.18.self_attn.o_proj.weight\",\n",
      "    \"model.layers.18.self_attn.q_proj.weight\",\n",
      "    \"model.layers.18.self_attn.v_proj.weight\",\n",
      "    \"model.layers.19.input_layernorm.weight\",\n",
      "    \"model.layers.19.mlp.down_proj.weight\",\n",
      "    \"model.layers.19.mlp.gate_proj.weight\",\n",
      "    \"model.layers.19.mlp.up_proj.weight\",\n",
      "    \"model.layers.19.post_attention_layernorm.weight\",\n",
      "    \"model.layers.19.self_attn.k_proj.weight\",\n",
      "    \"model.layers.19.self_attn.o_proj.weight\",\n",
      "    \"model.layers.19.self_attn.q_proj.weight\",\n",
      "    \"model.layers.19.self_attn.v_proj.weight\",\n",
      "    \"model.layers.2.input_layernorm.weight\",\n",
      "    \"model.layers.2.mlp.down_proj.weight\",\n",
      "    \"model.layers.2.mlp.gate_proj.weight\",\n",
      "    \"model.layers.2.mlp.up_proj.weight\",\n",
      "    \"model.layers.2.post_attention_layernorm.weight\",\n",
      "    \"model.layers.2.self_attn.k_proj.weight\",\n",
      "    \"model.layers.2.self_attn.o_proj.weight\",\n",
      "    \"model.layers.2.self_attn.q_proj.weight\",\n",
      "    \"model.layers.2.self_attn.v_proj.weight\",\n",
      "    \"model.layers.20.mlp.gate_proj.weight\",\n",
      "    \"model.layers.20.mlp.up_proj.weight\",\n",
      "    \"model.layers.20.self_attn.k_proj.weight\",\n",
      "    \"model.layers.20.self_attn.o_proj.weight\",\n",
      "    \"model.layers.20.self_attn.q_proj.weight\",\n",
      "    \"model.layers.20.self_attn.v_proj.weight\",\n",
      "    \"model.layers.3.input_layernorm.weight\",\n",
      "    \"model.layers.3.mlp.down_proj.weight\",\n",
      "    \"model.layers.3.mlp.gate_proj.weight\",\n",
      "    \"model.layers.3.mlp.up_proj.weight\",\n",
      "    \"model.layers.3.post_attention_layernorm.weight\",\n",
      "    \"model.layers.3.self_attn.k_proj.weight\",\n",
      "    \"model.layers.3.self_attn.o_proj.weight\",\n",
      "    \"model.layers.3.self_attn.q_proj.weight\",\n",
      "    \"model.layers.3.self_attn.v_proj.weight\",\n",
      "    \"model.layers.4.input_layernorm.weight\",\n",
      "    \"model.layers.4.mlp.down_proj.weight\",\n",
      "    \"model.layers.4.mlp.gate_proj.weight\",\n",
      "    \"model.layers.4.mlp.up_proj.weight\",\n",
      "    \"model.layers.4.post_attention_layernorm.weight\",\n",
      "    \"model.layers.4.self_attn.k_proj.weight\",\n",
      "    \"model.layers.4.self_attn.o_proj.weight\",\n",
      "    \"model.layers.4.self_attn.q_proj.weight\",\n",
      "    \"model.layers.4.self_attn.v_proj.weight\",\n",
      "    \"model.layers.5.input_layernorm.weight\",\n",
      "    \"model.layers.5.mlp.down_proj.weight\",\n",
      "    \"model.layers.5.mlp.gate_proj.weight\",\n",
      "    \"model.layers.5.mlp.up_proj.weight\",\n",
      "    \"model.layers.5.post_attention_layernorm.weight\",\n",
      "    \"model.layers.5.self_attn.k_proj.weight\",\n",
      "    \"model.layers.5.self_attn.o_proj.weight\",\n",
      "    \"model.layers.5.self_attn.q_proj.weight\",\n",
      "    \"model.layers.5.self_attn.v_proj.weight\",\n",
      "    \"model.layers.6.input_layernorm.weight\",\n",
      "    \"model.layers.6.mlp.down_proj.weight\",\n",
      "    \"model.layers.6.mlp.gate_proj.weight\",\n",
      "    \"model.layers.6.mlp.up_proj.weight\",\n",
      "    \"model.layers.6.post_attention_layernorm.weight\",\n",
      "    \"model.layers.6.self_attn.k_proj.weight\",\n",
      "    \"model.layers.6.self_attn.o_proj.weight\",\n",
      "    \"model.layers.6.self_attn.q_proj.weight\",\n",
      "    \"model.layers.6.self_attn.v_proj.weight\",\n",
      "    \"model.layers.7.input_layernorm.weight\",\n",
      "    \"model.layers.7.mlp.down_proj.weight\",\n",
      "    \"model.layers.7.mlp.gate_proj.weight\",\n",
      "    \"model.layers.7.mlp.up_proj.weight\",\n",
      "    \"model.layers.7.post_attention_layernorm.weight\",\n",
      "    \"model.layers.7.self_attn.k_proj.weight\",\n",
      "    \"model.layers.7.self_attn.o_proj.weight\",\n",
      "    \"model.layers.7.self_attn.q_proj.weight\",\n",
      "    \"model.layers.7.self_attn.v_proj.weight\",\n",
      "    \"model.layers.8.input_layernorm.weight\",\n",
      "    \"model.layers.8.mlp.down_proj.weight\",\n",
      "    \"model.layers.8.mlp.gate_proj.weight\",\n",
      "    \"model.layers.8.mlp.up_proj.weight\",\n",
      "    \"model.layers.8.post_attention_layernorm.weight\",\n",
      "    \"model.layers.8.self_attn.k_proj.weight\",\n",
      "    \"model.layers.8.self_attn.o_proj.weight\",\n",
      "    \"model.layers.8.self_attn.q_proj.weight\",\n",
      "    \"model.layers.8.self_attn.v_proj.weight\",\n",
      "    \"model.layers.9.input_layernorm.weight\",\n",
      "    \"model.layers.9.mlp.down_proj.weight\",\n",
      "    \"model.layers.9.mlp.gate_proj.weight\",\n",
      "    \"model.layers.9.mlp.up_proj.weight\",\n",
      "    \"model.layers.9.post_attention_layernorm.weight\",\n",
      "    \"model.layers.9.self_attn.k_proj.weight\",\n",
      "    \"model.layers.9.self_attn.o_proj.weight\",\n",
      "    \"model.layers.9.self_attn.q_proj.weight\",\n",
      "    \"model.layers.9.self_attn.v_proj.weight\"\n",
      "]\n",
      "[\n",
      "    \"model.layers.20.input_layernorm.weight\",\n",
      "    \"model.layers.20.mlp.down_proj.weight\",\n",
      "    \"model.layers.20.post_attention_layernorm.weight\",\n",
      "    \"model.layers.21.input_layernorm.weight\",\n",
      "    \"model.layers.21.mlp.down_proj.weight\",\n",
      "    \"model.layers.21.mlp.gate_proj.weight\",\n",
      "    \"model.layers.21.mlp.up_proj.weight\",\n",
      "    \"model.layers.21.post_attention_layernorm.weight\",\n",
      "    \"model.layers.21.self_attn.k_proj.weight\",\n",
      "    \"model.layers.21.self_attn.o_proj.weight\",\n",
      "    \"model.layers.21.self_attn.q_proj.weight\",\n",
      "    \"model.layers.21.self_attn.v_proj.weight\",\n",
      "    \"model.layers.22.input_layernorm.weight\",\n",
      "    \"model.layers.22.mlp.down_proj.weight\",\n",
      "    \"model.layers.22.mlp.gate_proj.weight\",\n",
      "    \"model.layers.22.mlp.up_proj.weight\",\n",
      "    \"model.layers.22.post_attention_layernorm.weight\",\n",
      "    \"model.layers.22.self_attn.k_proj.weight\",\n",
      "    \"model.layers.22.self_attn.o_proj.weight\",\n",
      "    \"model.layers.22.self_attn.q_proj.weight\",\n",
      "    \"model.layers.22.self_attn.v_proj.weight\",\n",
      "    \"model.layers.23.input_layernorm.weight\",\n",
      "    \"model.layers.23.mlp.down_proj.weight\",\n",
      "    \"model.layers.23.mlp.gate_proj.weight\",\n",
      "    \"model.layers.23.mlp.up_proj.weight\",\n",
      "    \"model.layers.23.post_attention_layernorm.weight\",\n",
      "    \"model.layers.23.self_attn.k_proj.weight\",\n",
      "    \"model.layers.23.self_attn.o_proj.weight\",\n",
      "    \"model.layers.23.self_attn.q_proj.weight\",\n",
      "    \"model.layers.23.self_attn.v_proj.weight\",\n",
      "    \"model.layers.24.input_layernorm.weight\",\n",
      "    \"model.layers.24.mlp.down_proj.weight\",\n",
      "    \"model.layers.24.mlp.gate_proj.weight\",\n",
      "    \"model.layers.24.mlp.up_proj.weight\",\n",
      "    \"model.layers.24.post_attention_layernorm.weight\",\n",
      "    \"model.layers.24.self_attn.k_proj.weight\",\n",
      "    \"model.layers.24.self_attn.o_proj.weight\",\n",
      "    \"model.layers.24.self_attn.q_proj.weight\",\n",
      "    \"model.layers.24.self_attn.v_proj.weight\",\n",
      "    \"model.layers.25.input_layernorm.weight\",\n",
      "    \"model.layers.25.mlp.down_proj.weight\",\n",
      "    \"model.layers.25.mlp.gate_proj.weight\",\n",
      "    \"model.layers.25.mlp.up_proj.weight\",\n",
      "    \"model.layers.25.post_attention_layernorm.weight\",\n",
      "    \"model.layers.25.self_attn.k_proj.weight\",\n",
      "    \"model.layers.25.self_attn.o_proj.weight\",\n",
      "    \"model.layers.25.self_attn.q_proj.weight\",\n",
      "    \"model.layers.25.self_attn.v_proj.weight\",\n",
      "    \"model.layers.26.input_layernorm.weight\",\n",
      "    \"model.layers.26.mlp.down_proj.weight\",\n",
      "    \"model.layers.26.mlp.gate_proj.weight\",\n",
      "    \"model.layers.26.mlp.up_proj.weight\",\n",
      "    \"model.layers.26.post_attention_layernorm.weight\",\n",
      "    \"model.layers.26.self_attn.k_proj.weight\",\n",
      "    \"model.layers.26.self_attn.o_proj.weight\",\n",
      "    \"model.layers.26.self_attn.q_proj.weight\",\n",
      "    \"model.layers.26.self_attn.v_proj.weight\",\n",
      "    \"model.layers.27.input_layernorm.weight\",\n",
      "    \"model.layers.27.mlp.down_proj.weight\",\n",
      "    \"model.layers.27.mlp.gate_proj.weight\",\n",
      "    \"model.layers.27.mlp.up_proj.weight\",\n",
      "    \"model.layers.27.post_attention_layernorm.weight\",\n",
      "    \"model.layers.27.self_attn.k_proj.weight\",\n",
      "    \"model.layers.27.self_attn.o_proj.weight\",\n",
      "    \"model.layers.27.self_attn.q_proj.weight\",\n",
      "    \"model.layers.27.self_attn.v_proj.weight\",\n",
      "    \"model.norm.weight\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 读取模型权重\n",
    "from safetensors import safe_open\n",
    "weights_root = \"/HOME/scz0101/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95\"\n",
    "# 文件路径\n",
    "file1 = os.path.join(weights_root, \"model-00001-of-00002.safetensors\")\n",
    "file2 = os.path.join(weights_root, \"model-00002-of-00002.safetensors\")\n",
    "\n",
    "# # 加载第一个文件\n",
    "with safe_open(file1, framework=\"pt\", device=\"cpu\") as f:\n",
    "    state_dict1 = {key: f.get_tensor(key) for key in f.keys()}\n",
    "# # 查看key和size\n",
    "print(json.dumps(list(state_dict1.keys()), indent=4)) ## layer0~20\n",
    "# print(\"model.embed_tokens.weight\", state_dict1[\"model.embed_tokens.weight\"].shape) # vob_size*hidden_size\n",
    "# print(\"model.layers.0.input_layernorm.weight\", state_dict1[\"model.layers.0.input_layernorm.weight\"].shape) #hidden_size\n",
    "# print(\"model.layers.0.mlp.down_proj.weight\", state_dict1[\"model.layers.0.mlp.down_proj.weight\"].shape) #hidden_size * intermediate_size\n",
    "# print(\"model.layers.0.mlp.gate_proj.weight\", state_dict1[\"model.layers.0.mlp.gate_proj.weight\"].shape) # intermediate_size * hidden_size \n",
    "# print(\"model.layers.0.mlp.up_proj.weight\", state_dict1[\"model.layers.0.mlp.up_proj.weight\"].shape) #intermediate_size * hidden_size \n",
    "# print(\"model.layers.0.post_attention_layernorm.weight\",state_dict1[\"model.layers.0.post_attention_layernorm.weight\"].shape) #hidden_size\n",
    "# print(\"model.layers.0.self_attn.k_proj.weight\",state_dict1[\"model.layers.0.self_attn.k_proj.weight\"].shape) #(head_dim*num_key_value_heads)*hidden_size\n",
    "# print(\"model.layers.0.self_attn.o_proj.weight\",state_dict1[\"model.layers.0.self_attn.o_proj.weight\"].shape) #(head_dim*num_attention_heads)*hidden_size\n",
    "# print(\"model.layers.0.self_attn.q_proj.weight\",state_dict1[\"model.layers.0.self_attn.q_proj.weight\"].shape) #(head_dim*num_attention_heads)*hidden_size\n",
    "# print(\"model.layers.0.self_attn.v_proj.weight\",state_dict1[\"model.layers.0.self_attn.v_proj.weight\"].shape) #(head_dim*num_key_value_heads)*hidden_size\n",
    "\n",
    "# # # 加载第二个文件(为了节省时间，可以先skip)\n",
    "with safe_open(file2, framework=\"pt\", device=\"cpu\") as f:\n",
    "    state_dict2 = {key: f.get_tensor(key) for key in f.keys()}\n",
    "\n",
    "# # 合并两个状态字典\n",
    "# state_dict = {**state_dict1, **state_dict2}\n",
    "\n",
    "# # 查看模型的权重key\n",
    "print(json.dumps(list(state_dict2.keys()), indent=4)) ## layer21~27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先跳过实现tokenizer，可以直接使用autotokenizer，后续再来实现\n",
    "model_name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,   5269,    527,    499,     30]])\n",
      "how are you?\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 创建输入\n",
    "input_text = \"how are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(inputs[\"input_ids\"])\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0].tolist(), skip_special_tokens=True))\n",
    "seq_len = len(inputs[\"input_ids\"][0].tolist()) ## token num\n",
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1292e-02,  9.9487e-03,  1.4160e-02,  ..., -3.5706e-03,\n",
       "         -1.9775e-02,  5.3711e-03],\n",
       "        [ 1.3245e-02, -3.8385e-05,  2.2461e-02,  ..., -2.6550e-03,\n",
       "          3.1738e-02, -1.0681e-03],\n",
       "        [ 1.9775e-02,  2.0020e-02,  2.8687e-02,  ..., -3.5248e-03,\n",
       "          3.1433e-03, -7.6294e-03],\n",
       "        ...,\n",
       "        [-3.0975e-03,  2.1057e-03,  4.8828e-03,  ..., -2.0905e-03,\n",
       "         -1.2207e-03, -2.8992e-03],\n",
       "        [-3.0975e-03,  2.1057e-03,  4.8828e-03,  ..., -2.0905e-03,\n",
       "         -1.2207e-03, -2.8992e-03],\n",
       "        [-3.0975e-03,  2.1057e-03,  4.8828e-03,  ..., -2.0905e-03,\n",
       "         -1.2207e-03, -2.8992e-03]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#开始创建embeding_layer\n",
    "embedding_layer = torch.nn.Embedding(config['vocab_size'], config[\"hidden_size\"])\n",
    "embedding_layer.weight.data.copy_(state_dict1[\"model.embed_tokens.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建rms_norm层，tensor2维，对每一行做归一化\n",
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备位置编码（RoPE）\n",
    "import math\n",
    "\n",
    "def get_inv_freq_llama3(device):\n",
    "    dim = config[\"head_dim\"]\n",
    "    rope_theta = config[\"rope_theta\"]\n",
    "    inv_freq = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n",
    "    # print(inv_freq)\n",
    "    attention_factor = 1.0\n",
    "    # 对于llama3的特殊处理\n",
    "    factor = config[\"rope_scaling\"][\"factor\"]  # `8` in the original implementation\n",
    "    low_freq_factor = config[\"rope_scaling\"][\"low_freq_factor\"]  # `1` in the original implementation\n",
    "    high_freq_factor = config[\"rope_scaling\"][\"high_freq_factor\"]  # `4` in the original implementation\n",
    "    old_context_len = config[\"rope_scaling\"][\"original_max_position_embeddings\"]  # `8192` in the original implementation\n",
    "\n",
    "    low_freq_wavelen = old_context_len / low_freq_factor\n",
    "    high_freq_wavelen = old_context_len / high_freq_factor\n",
    "\n",
    "\n",
    "\n",
    "    wavelen = 2 * math.pi / inv_freq\n",
    "\n",
    "    print(low_freq_wavelen, high_freq_wavelen, wavelen)\n",
    "    # wavelen < high_freq_wavelen: do nothing\n",
    "    # wavelen > low_freq_wavelen: divide by factor\n",
    "    inv_freq_llama = torch.where(wavelen > low_freq_wavelen, inv_freq / factor, inv_freq)\n",
    "    # otherwise: interpolate between the two, using a smooth factor\n",
    "    smooth_factor = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n",
    "    smoothed_inv_freq = (1 - smooth_factor) * inv_freq_llama / factor + smooth_factor * inv_freq_llama\n",
    "    is_medium_freq = ~(wavelen < high_freq_wavelen) * ~(wavelen > low_freq_wavelen)\n",
    "    inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
    "    return inv_freq_llama, attention_factor\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "    参考：https://blog.csdn.net/UCB001/article/details/139511775\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.intermediate_size = config[\"intermediate_size\"]\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config[\"mlp_bias\"])\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config[\"mlp_bias\"])\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config[\"mlp_bias\"])\n",
    "        self.act_fn = nn.SiLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出是否接近: True\n",
      "标准 SDPA 时间: 3.444230 秒\n",
      "Flash Attention 时间: 0.586423 秒\n"
     ]
    }
   ],
   "source": [
    "## 番外，对于普通sdpa和flash atten\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 定义输入参数\n",
    "batch_size = 8\n",
    "num_heads = 8\n",
    "seq_len = 1024\n",
    "d_model = 64\n",
    "\n",
    "# 随机生成 Q, K, V\n",
    "Q = torch.randn(batch_size, num_heads, seq_len, d_model)\n",
    "K = torch.randn(batch_size, num_heads, seq_len, d_model)\n",
    "V = torch.randn(batch_size, num_heads, seq_len, d_model)\n",
    "\n",
    "# 标准 SDPA 实现\n",
    "def standard_sdpa(Q, K, V):\n",
    "    scale = d_model ** 0.5\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / scale  # QK^T / sqrt(d_k)\n",
    "    attn_weights = F.softmax(scores, dim=-1)  # Softmax\n",
    "    output = torch.matmul(attn_weights, V)  # 加权求和\n",
    "    return output\n",
    "\n",
    "# 使用 PyTorch 的优化实现（基于 Flash Attention）\n",
    "def flash_attention(Q, K, V):\n",
    "    return F.scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# 测试标准 SDPA\n",
    "start_time = time.time()\n",
    "output_standard = standard_sdpa(Q, K, V)\n",
    "standard_time = time.time() - start_time\n",
    "\n",
    "# 测试 Flash Attention\n",
    "start_time = time.time()\n",
    "output_flash = flash_attention(Q, K, V)\n",
    "flash_time = time.time() - start_time\n",
    "\n",
    "# 检查输出是否一致（允许微小误差）\n",
    "print(\"输出是否接近:\", torch.allclose(output_standard, output_flash, atol=1e-5))\n",
    "print(f\"标准 SDPA 时间: {standard_time:.6f} 秒\")\n",
    "print(f\"Flash Attention 时间: {flash_time:.6f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "import torch\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def sdpa_attention_forward(\n",
    "    module: torch.nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    dropout: float = 0.0,\n",
    "    scaling: Optional[float] = None,\n",
    "    is_causal: Optional[bool] = None\n",
    ") -> Tuple[torch.Tensor, None]:\n",
    "    if hasattr(module, \"num_key_value_groups\"):\n",
    "        key = repeat_kv(key, module.num_key_value_groups)\n",
    "        value = repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n",
    "\n",
    "    # SDPA with memory-efficient backend is bugged with non-contiguous inputs and custom attn_mask for some torch versions\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
    "    query = query.contiguous()\n",
    "    key = key.contiguous()\n",
    "    value = value.contiguous()\n",
    "\n",
    "    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n",
    "    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n",
    "    if is_causal is None:\n",
    "        is_causal = causal_mask is None and query.shape[2] > 1\n",
    "\n",
    "    # Shapes (e.g. query.shape[2]) are tensors during jit tracing, resulting in `is_causal` being a tensor.\n",
    "    # We convert it to a bool for the SDPA kernel that only accepts bools.\n",
    "    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n",
    "        is_causal = is_causal.item()\n",
    "\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=dropout,\n",
    "        scale=scaling,\n",
    "        is_causal=is_causal,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, None\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = config[\"head_dim\"]\n",
    "        self.num_key_value_groups = config[\"num_attention_heads\"] // config[\"num_key_value_heads\"]\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.attention_dropout = config['attention_dropout']\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config['hidden_size'], config['num_attention_heads'] * self.head_dim, bias=config['attention_bias']\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config['hidden_size'], config['num_key_value_heads'] * self.head_dim, bias=config['attention_bias']\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config['hidden_size'], config['num_key_value_heads'] * self.head_dim, bias=config['attention_bias']\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config['num_attention_heads'] * self.head_dim, config['hidden_size'], bias=config['attention_bias']\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        attn_output, attn_weights = sdpa_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "\n",
    "        self.self_attn = LlamaAttention(layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP()\n",
    "        self.input_layernorm = LlamaRMSNorm(config['hidden_size'], eps=config['rms_norm_eps'])\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config['hidden_size'], eps=config['rms_norm_eps'])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None # necessary, but kept here for BC\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_embeddings=position_embeddings\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        # if output_attentions:\n",
    "        #     outputs += (self_attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__()\n",
    "        # BC: \"rope_type\" was originally \"type\"\n",
    "\n",
    "        self.rope_type = config['rope_scaling']['rope_type']\n",
    "        self.max_seq_len_cached = config['max_position_embeddings']\n",
    "        self.original_max_seq_len = config['max_position_embeddings']\n",
    "\n",
    "        self.config = config\n",
    "        self.rope_init_fn = get_inv_freq_llama3\n",
    "\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        # Core RoPE block\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False): #enabled=False表示在此上下文中禁用混合精度，所有计算将使用默认的浮点精度（通常是FP32）\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "\n",
    "        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n",
    "        cos = cos * self.attention_scaling\n",
    "        sin = sin * self.attention_scaling\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.padding_idx = config['pad_token_id']\n",
    "        self.vocab_size = config['vocab_size']\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(self.vocab_size, config['hidden_size'])\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(layer_idx) for layer_idx in range(config['num_hidden_layers'])]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config['hidden_size'], eps=config['rms_norm_eps'])\n",
    "        self.rotary_emb = LlamaRotaryEmbedding()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None\n",
    "    ):\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_embeddings=position_embeddings\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel()\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.lm_head = nn.Linear(config['hidden_size'], config['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0\n",
    "    ):\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192.0 2048.0 tensor([6.2832e+00, 7.7131e+00, 9.4683e+00, 1.1623e+01, 1.4268e+01, 1.7515e+01,\n",
      "        2.1501e+01, 2.6394e+01, 3.2400e+01, 3.9774e+01, 4.8825e+01, 5.9936e+01,\n",
      "        7.3576e+01, 9.0320e+01, 1.1087e+02, 1.3611e+02, 1.6708e+02, 2.0510e+02,\n",
      "        2.5178e+02, 3.0907e+02, 3.7941e+02, 4.6575e+02, 5.7174e+02, 7.0185e+02,\n",
      "        8.6158e+02, 1.0576e+03, 1.2983e+03, 1.5938e+03, 1.9565e+03, 2.4017e+03,\n",
      "        2.9483e+03, 3.6192e+03, 4.4429e+03, 5.4540e+03, 6.6951e+03, 8.2187e+03,\n",
      "        1.0089e+04, 1.2385e+04, 1.5203e+04, 1.8663e+04, 2.2911e+04, 2.8124e+04,\n",
      "        3.4525e+04, 4.2381e+04, 5.2026e+04, 6.3866e+04, 7.8400e+04, 9.6241e+04,\n",
      "        1.1814e+05, 1.4503e+05, 1.7803e+05, 2.1855e+05, 2.6828e+05, 3.2934e+05,\n",
      "        4.0428e+05, 4.9629e+05, 6.0923e+05, 7.4787e+05, 9.1806e+05, 1.1270e+06,\n",
      "        1.3835e+06, 1.6983e+06, 2.0848e+06, 2.5592e+06])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LlamaForCausalLM:\n\tMissing key(s) in state_dict: \"lm_head.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_part1, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_part2}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 加载合并后的状态字典\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/model_acc/lib/python3.10/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LlamaForCausalLM:\n\tMissing key(s) in state_dict: \"lm_head.weight\". "
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "model = LlamaForCausalLM()\n",
    "\n",
    "# for name, module in model.named_modules():\n",
    "#     print(f\"层名称：{name}\")\n",
    "#     print(f\"参数数量：{sum(p.numel() for p in module.parameters())}\")\n",
    "from safetensors.torch import load_file\n",
    "# 加载第一个分片\n",
    "model_part1 = load_file(file1)\n",
    "\n",
    "# 加载第二个分片\n",
    "model_part2 = load_file(file2)\n",
    "\n",
    "# 合并模型参数\n",
    "model_state_dict = {**model_part1, **model_part2}\n",
    "\n",
    "# 加载合并后的状态字典\n",
    "model.load_state_dict(model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(logits, temperature: float = 1.0):\n",
    "    \"\"\"\n",
    "    Samples a token from the logits using temperature scaling.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): The logits tensor for token predictions.\n",
    "        temperature (float, optional): Temperature for scaling logits. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The sampled token.\n",
    "    \"\"\"\n",
    "    logits = logits / max(temperature, 1e-5)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    return probs.div_(torch.empty_like(probs).exponential_(1)).argmax(dim=-1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    model: LlamaForCausalLM,\n",
    "    prompt_tokens: List[List[int]],\n",
    "    max_new_tokens: int,\n",
    "    eos_id: int,\n",
    "    temperature: float = 1.0\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Generates new tokens based on the given prompt tokens using the specified model.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): The transformer model used for token generation.\n",
    "        prompt_tokens (List[List[int]]): A list of lists containing the prompt tokens for each sequence.\n",
    "        max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "        eos_id (int): The end-of-sequence token ID.\n",
    "        temperature (float, optional): The temperature value for sampling. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: A list of lists containing the generated tokens for each sequence.\n",
    "    \"\"\"\n",
    "    prompt_lens = [len(t) for t in prompt_tokens]\n",
    "    assert max(prompt_lens) <= model.max_seq_len\n",
    "    total_len = min(model.max_seq_len, max_new_tokens + max(prompt_lens))\n",
    "    tokens = torch.full((len(prompt_tokens), total_len), -1, dtype=torch.long, device=\"cuda\")\n",
    "    for i, t in enumerate(prompt_tokens):\n",
    "        tokens[i, :len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "    prev_pos = 0\n",
    "    finished = torch.tensor([False] * len(prompt_tokens), device=\"cuda\")\n",
    "    prompt_mask = tokens != -1\n",
    "    for cur_pos in range(min(prompt_lens), total_len):\n",
    "        logits = model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "        if temperature > 0:\n",
    "            next_token = sample(logits, temperature)\n",
    "        else:\n",
    "            next_token = logits.argmax(dim=-1)\n",
    "        next_token = torch.where(prompt_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        finished |= torch.logical_and(~prompt_mask[:, cur_pos], next_token == eos_id)\n",
    "        prev_pos = cur_pos\n",
    "        if finished.all():\n",
    "            break\n",
    "    completion_tokens = []\n",
    "    for i, toks in enumerate(tokens.tolist()):\n",
    "        toks = toks[prompt_lens[i]:prompt_lens[i]+max_new_tokens]\n",
    "        if eos_id in toks:\n",
    "            toks = toks[:toks.index(eos_id)]\n",
    "        completion_tokens.append(toks)\n",
    "    return completion_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_acc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
