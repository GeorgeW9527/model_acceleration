{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#llama3.2 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architectures': ['LlamaForCausalLM'], 'attention_bias': False, 'attention_dropout': 0.0, 'bos_token_id': 128000, 'eos_token_id': [128001, 128008, 128009], 'head_dim': 128, 'hidden_act': 'silu', 'hidden_size': 3072, 'initializer_range': 0.02, 'intermediate_size': 8192, 'max_position_embeddings': 131072, 'mlp_bias': False, 'model_type': 'llama', 'num_attention_heads': 24, 'num_hidden_layers': 28, 'num_key_value_heads': 8, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': {'factor': 32.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}, 'rope_theta': 500000.0, 'tie_word_embeddings': True, 'torch_dtype': 'bfloat16', 'transformers_version': '4.45.0.dev0', 'use_cache': True, 'vocab_size': 128256}\n"
     ]
    }
   ],
   "source": [
    "# 读取并查看模型参数\n",
    "with open(\"/HOME/scz0101/run/model_acceleration/models/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"model.embed_tokens.weight\",\n",
      "    \"model.layers.0.input_layernorm.weight\",\n",
      "    \"model.layers.0.mlp.down_proj.weight\",\n",
      "    \"model.layers.0.mlp.gate_proj.weight\",\n",
      "    \"model.layers.0.mlp.up_proj.weight\",\n",
      "    \"model.layers.0.post_attention_layernorm.weight\",\n",
      "    \"model.layers.0.self_attn.k_proj.weight\",\n",
      "    \"model.layers.0.self_attn.o_proj.weight\",\n",
      "    \"model.layers.0.self_attn.q_proj.weight\",\n",
      "    \"model.layers.0.self_attn.v_proj.weight\",\n",
      "    \"model.layers.1.input_layernorm.weight\",\n",
      "    \"model.layers.1.mlp.down_proj.weight\",\n",
      "    \"model.layers.1.mlp.gate_proj.weight\",\n",
      "    \"model.layers.1.mlp.up_proj.weight\",\n",
      "    \"model.layers.1.post_attention_layernorm.weight\",\n",
      "    \"model.layers.1.self_attn.k_proj.weight\",\n",
      "    \"model.layers.1.self_attn.o_proj.weight\",\n",
      "    \"model.layers.1.self_attn.q_proj.weight\",\n",
      "    \"model.layers.1.self_attn.v_proj.weight\",\n",
      "    \"model.layers.10.input_layernorm.weight\",\n",
      "    \"model.layers.10.mlp.down_proj.weight\",\n",
      "    \"model.layers.10.mlp.gate_proj.weight\",\n",
      "    \"model.layers.10.mlp.up_proj.weight\",\n",
      "    \"model.layers.10.post_attention_layernorm.weight\",\n",
      "    \"model.layers.10.self_attn.k_proj.weight\",\n",
      "    \"model.layers.10.self_attn.o_proj.weight\",\n",
      "    \"model.layers.10.self_attn.q_proj.weight\",\n",
      "    \"model.layers.10.self_attn.v_proj.weight\",\n",
      "    \"model.layers.11.input_layernorm.weight\",\n",
      "    \"model.layers.11.mlp.down_proj.weight\",\n",
      "    \"model.layers.11.mlp.gate_proj.weight\",\n",
      "    \"model.layers.11.mlp.up_proj.weight\",\n",
      "    \"model.layers.11.post_attention_layernorm.weight\",\n",
      "    \"model.layers.11.self_attn.k_proj.weight\",\n",
      "    \"model.layers.11.self_attn.o_proj.weight\",\n",
      "    \"model.layers.11.self_attn.q_proj.weight\",\n",
      "    \"model.layers.11.self_attn.v_proj.weight\",\n",
      "    \"model.layers.12.input_layernorm.weight\",\n",
      "    \"model.layers.12.mlp.down_proj.weight\",\n",
      "    \"model.layers.12.mlp.gate_proj.weight\",\n",
      "    \"model.layers.12.mlp.up_proj.weight\",\n",
      "    \"model.layers.12.post_attention_layernorm.weight\",\n",
      "    \"model.layers.12.self_attn.k_proj.weight\",\n",
      "    \"model.layers.12.self_attn.o_proj.weight\",\n",
      "    \"model.layers.12.self_attn.q_proj.weight\",\n",
      "    \"model.layers.12.self_attn.v_proj.weight\",\n",
      "    \"model.layers.13.input_layernorm.weight\",\n",
      "    \"model.layers.13.mlp.down_proj.weight\",\n",
      "    \"model.layers.13.mlp.gate_proj.weight\",\n",
      "    \"model.layers.13.mlp.up_proj.weight\",\n",
      "    \"model.layers.13.post_attention_layernorm.weight\",\n",
      "    \"model.layers.13.self_attn.k_proj.weight\",\n",
      "    \"model.layers.13.self_attn.o_proj.weight\",\n",
      "    \"model.layers.13.self_attn.q_proj.weight\",\n",
      "    \"model.layers.13.self_attn.v_proj.weight\",\n",
      "    \"model.layers.14.input_layernorm.weight\",\n",
      "    \"model.layers.14.mlp.down_proj.weight\",\n",
      "    \"model.layers.14.mlp.gate_proj.weight\",\n",
      "    \"model.layers.14.mlp.up_proj.weight\",\n",
      "    \"model.layers.14.post_attention_layernorm.weight\",\n",
      "    \"model.layers.14.self_attn.k_proj.weight\",\n",
      "    \"model.layers.14.self_attn.o_proj.weight\",\n",
      "    \"model.layers.14.self_attn.q_proj.weight\",\n",
      "    \"model.layers.14.self_attn.v_proj.weight\",\n",
      "    \"model.layers.15.input_layernorm.weight\",\n",
      "    \"model.layers.15.mlp.down_proj.weight\",\n",
      "    \"model.layers.15.mlp.gate_proj.weight\",\n",
      "    \"model.layers.15.mlp.up_proj.weight\",\n",
      "    \"model.layers.15.post_attention_layernorm.weight\",\n",
      "    \"model.layers.15.self_attn.k_proj.weight\",\n",
      "    \"model.layers.15.self_attn.o_proj.weight\",\n",
      "    \"model.layers.15.self_attn.q_proj.weight\",\n",
      "    \"model.layers.15.self_attn.v_proj.weight\",\n",
      "    \"model.layers.16.input_layernorm.weight\",\n",
      "    \"model.layers.16.mlp.down_proj.weight\",\n",
      "    \"model.layers.16.mlp.gate_proj.weight\",\n",
      "    \"model.layers.16.mlp.up_proj.weight\",\n",
      "    \"model.layers.16.post_attention_layernorm.weight\",\n",
      "    \"model.layers.16.self_attn.k_proj.weight\",\n",
      "    \"model.layers.16.self_attn.o_proj.weight\",\n",
      "    \"model.layers.16.self_attn.q_proj.weight\",\n",
      "    \"model.layers.16.self_attn.v_proj.weight\",\n",
      "    \"model.layers.17.input_layernorm.weight\",\n",
      "    \"model.layers.17.mlp.down_proj.weight\",\n",
      "    \"model.layers.17.mlp.gate_proj.weight\",\n",
      "    \"model.layers.17.mlp.up_proj.weight\",\n",
      "    \"model.layers.17.post_attention_layernorm.weight\",\n",
      "    \"model.layers.17.self_attn.k_proj.weight\",\n",
      "    \"model.layers.17.self_attn.o_proj.weight\",\n",
      "    \"model.layers.17.self_attn.q_proj.weight\",\n",
      "    \"model.layers.17.self_attn.v_proj.weight\",\n",
      "    \"model.layers.18.input_layernorm.weight\",\n",
      "    \"model.layers.18.mlp.down_proj.weight\",\n",
      "    \"model.layers.18.mlp.gate_proj.weight\",\n",
      "    \"model.layers.18.mlp.up_proj.weight\",\n",
      "    \"model.layers.18.post_attention_layernorm.weight\",\n",
      "    \"model.layers.18.self_attn.k_proj.weight\",\n",
      "    \"model.layers.18.self_attn.o_proj.weight\",\n",
      "    \"model.layers.18.self_attn.q_proj.weight\",\n",
      "    \"model.layers.18.self_attn.v_proj.weight\",\n",
      "    \"model.layers.19.input_layernorm.weight\",\n",
      "    \"model.layers.19.mlp.down_proj.weight\",\n",
      "    \"model.layers.19.mlp.gate_proj.weight\",\n",
      "    \"model.layers.19.mlp.up_proj.weight\",\n",
      "    \"model.layers.19.post_attention_layernorm.weight\",\n",
      "    \"model.layers.19.self_attn.k_proj.weight\",\n",
      "    \"model.layers.19.self_attn.o_proj.weight\",\n",
      "    \"model.layers.19.self_attn.q_proj.weight\",\n",
      "    \"model.layers.19.self_attn.v_proj.weight\",\n",
      "    \"model.layers.2.input_layernorm.weight\",\n",
      "    \"model.layers.2.mlp.down_proj.weight\",\n",
      "    \"model.layers.2.mlp.gate_proj.weight\",\n",
      "    \"model.layers.2.mlp.up_proj.weight\",\n",
      "    \"model.layers.2.post_attention_layernorm.weight\",\n",
      "    \"model.layers.2.self_attn.k_proj.weight\",\n",
      "    \"model.layers.2.self_attn.o_proj.weight\",\n",
      "    \"model.layers.2.self_attn.q_proj.weight\",\n",
      "    \"model.layers.2.self_attn.v_proj.weight\",\n",
      "    \"model.layers.20.mlp.gate_proj.weight\",\n",
      "    \"model.layers.20.mlp.up_proj.weight\",\n",
      "    \"model.layers.20.self_attn.k_proj.weight\",\n",
      "    \"model.layers.20.self_attn.o_proj.weight\",\n",
      "    \"model.layers.20.self_attn.q_proj.weight\",\n",
      "    \"model.layers.20.self_attn.v_proj.weight\",\n",
      "    \"model.layers.3.input_layernorm.weight\",\n",
      "    \"model.layers.3.mlp.down_proj.weight\",\n",
      "    \"model.layers.3.mlp.gate_proj.weight\",\n",
      "    \"model.layers.3.mlp.up_proj.weight\",\n",
      "    \"model.layers.3.post_attention_layernorm.weight\",\n",
      "    \"model.layers.3.self_attn.k_proj.weight\",\n",
      "    \"model.layers.3.self_attn.o_proj.weight\",\n",
      "    \"model.layers.3.self_attn.q_proj.weight\",\n",
      "    \"model.layers.3.self_attn.v_proj.weight\",\n",
      "    \"model.layers.4.input_layernorm.weight\",\n",
      "    \"model.layers.4.mlp.down_proj.weight\",\n",
      "    \"model.layers.4.mlp.gate_proj.weight\",\n",
      "    \"model.layers.4.mlp.up_proj.weight\",\n",
      "    \"model.layers.4.post_attention_layernorm.weight\",\n",
      "    \"model.layers.4.self_attn.k_proj.weight\",\n",
      "    \"model.layers.4.self_attn.o_proj.weight\",\n",
      "    \"model.layers.4.self_attn.q_proj.weight\",\n",
      "    \"model.layers.4.self_attn.v_proj.weight\",\n",
      "    \"model.layers.5.input_layernorm.weight\",\n",
      "    \"model.layers.5.mlp.down_proj.weight\",\n",
      "    \"model.layers.5.mlp.gate_proj.weight\",\n",
      "    \"model.layers.5.mlp.up_proj.weight\",\n",
      "    \"model.layers.5.post_attention_layernorm.weight\",\n",
      "    \"model.layers.5.self_attn.k_proj.weight\",\n",
      "    \"model.layers.5.self_attn.o_proj.weight\",\n",
      "    \"model.layers.5.self_attn.q_proj.weight\",\n",
      "    \"model.layers.5.self_attn.v_proj.weight\",\n",
      "    \"model.layers.6.input_layernorm.weight\",\n",
      "    \"model.layers.6.mlp.down_proj.weight\",\n",
      "    \"model.layers.6.mlp.gate_proj.weight\",\n",
      "    \"model.layers.6.mlp.up_proj.weight\",\n",
      "    \"model.layers.6.post_attention_layernorm.weight\",\n",
      "    \"model.layers.6.self_attn.k_proj.weight\",\n",
      "    \"model.layers.6.self_attn.o_proj.weight\",\n",
      "    \"model.layers.6.self_attn.q_proj.weight\",\n",
      "    \"model.layers.6.self_attn.v_proj.weight\",\n",
      "    \"model.layers.7.input_layernorm.weight\",\n",
      "    \"model.layers.7.mlp.down_proj.weight\",\n",
      "    \"model.layers.7.mlp.gate_proj.weight\",\n",
      "    \"model.layers.7.mlp.up_proj.weight\",\n",
      "    \"model.layers.7.post_attention_layernorm.weight\",\n",
      "    \"model.layers.7.self_attn.k_proj.weight\",\n",
      "    \"model.layers.7.self_attn.o_proj.weight\",\n",
      "    \"model.layers.7.self_attn.q_proj.weight\",\n",
      "    \"model.layers.7.self_attn.v_proj.weight\",\n",
      "    \"model.layers.8.input_layernorm.weight\",\n",
      "    \"model.layers.8.mlp.down_proj.weight\",\n",
      "    \"model.layers.8.mlp.gate_proj.weight\",\n",
      "    \"model.layers.8.mlp.up_proj.weight\",\n",
      "    \"model.layers.8.post_attention_layernorm.weight\",\n",
      "    \"model.layers.8.self_attn.k_proj.weight\",\n",
      "    \"model.layers.8.self_attn.o_proj.weight\",\n",
      "    \"model.layers.8.self_attn.q_proj.weight\",\n",
      "    \"model.layers.8.self_attn.v_proj.weight\",\n",
      "    \"model.layers.9.input_layernorm.weight\",\n",
      "    \"model.layers.9.mlp.down_proj.weight\",\n",
      "    \"model.layers.9.mlp.gate_proj.weight\",\n",
      "    \"model.layers.9.mlp.up_proj.weight\",\n",
      "    \"model.layers.9.post_attention_layernorm.weight\",\n",
      "    \"model.layers.9.self_attn.k_proj.weight\",\n",
      "    \"model.layers.9.self_attn.o_proj.weight\",\n",
      "    \"model.layers.9.self_attn.q_proj.weight\",\n",
      "    \"model.layers.9.self_attn.v_proj.weight\"\n",
      "]\n",
      "model.embed_tokens.weight torch.Size([128256, 3072])\n",
      "model.layers.0.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([8192, 3072])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([8192, 3072])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 3072])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 3072])\n"
     ]
    }
   ],
   "source": [
    "# 读取模型权重\n",
    "from safetensors import safe_open\n",
    "weights_root = \"/HOME/scz0101/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95\"\n",
    "# 文件路径\n",
    "file1 = os.path.join(weights_root, \"model-00001-of-00002.safetensors\")\n",
    "file2 = os.path.join(weights_root, \"model-00002-of-00002.safetensors\")\n",
    "\n",
    "# 加载第一个文件\n",
    "with safe_open(file1, framework=\"pt\", device=\"cpu\") as f:\n",
    "    state_dict1 = {key: f.get_tensor(key) for key in f.keys()}\n",
    "# 查看key和size\n",
    "print(json.dumps(list(state_dict1.keys()), indent=4)) ## layer0~20\n",
    "print(\"model.embed_tokens.weight\", state_dict1[\"model.embed_tokens.weight\"].shape) # vob_size*hidden_size\n",
    "print(\"model.layers.0.input_layernorm.weight\", state_dict1[\"model.layers.0.input_layernorm.weight\"].shape) #hidden_size\n",
    "print(\"model.layers.0.mlp.down_proj.weight\", state_dict1[\"model.layers.0.mlp.down_proj.weight\"].shape) #hidden_size * intermediate_size\n",
    "print(\"model.layers.0.mlp.gate_proj.weight\", state_dict1[\"model.layers.0.mlp.gate_proj.weight\"].shape) # intermediate_size * hidden_size \n",
    "print(\"model.layers.0.mlp.up_proj.weight\", state_dict1[\"model.layers.0.mlp.up_proj.weight\"].shape) #intermediate_size * hidden_size \n",
    "print(\"model.layers.0.post_attention_layernorm.weight\",state_dict1[\"model.layers.0.post_attention_layernorm.weight\"].shape) #hidden_size\n",
    "print(\"model.layers.0.self_attn.k_proj.weight\",state_dict1[\"model.layers.0.self_attn.k_proj.weight\"].shape) #(head_dim*num_key_value_heads)*hidden_size\n",
    "print(\"model.layers.0.self_attn.o_proj.weight\",state_dict1[\"model.layers.0.self_attn.o_proj.weight\"].shape) #(head_dim*num_attention_heads)*hidden_size\n",
    "print(\"model.layers.0.self_attn.q_proj.weight\",state_dict1[\"model.layers.0.self_attn.q_proj.weight\"].shape) #(head_dim*num_attention_heads)*hidden_size\n",
    "print(\"model.layers.0.self_attn.v_proj.weight\",state_dict1[\"model.layers.0.self_attn.v_proj.weight\"].shape) #(head_dim*num_key_value_heads)*hidden_size\n",
    "\n",
    "# # 加载第二个文件(为了节省时间，可以先skip)\n",
    "# with safe_open(file2, framework=\"pt\", device=\"cpu\") as f:\n",
    "#     state_dict2 = {key: f.get_tensor(key) for key in f.keys()}\n",
    "\n",
    "# # 合并两个状态字典\n",
    "# state_dict = {**state_dict1, **state_dict2}\n",
    "\n",
    "# # 查看模型的权重key\n",
    "# print(json.dumps(list(state_dict2.keys()), indent=4)) ## layer21~27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先跳过实现tokenizer，可以直接使用autotokenizer，后续再来实现\n",
    "model_name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,   5269,    527,    499,     30]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/model_acc/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3851\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3848\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3849\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3856\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/model_acc/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:668\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    667\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 668\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    671\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    674\u001b[0m )\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# 创建输入\n",
    "input_text = \"how are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(inputs[\"input_ids\"])\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0].tolist(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1292e-02,  9.9487e-03,  1.4160e-02,  ..., -3.5706e-03,\n",
       "         -1.9775e-02,  5.3711e-03],\n",
       "        [ 1.3245e-02, -3.8385e-05,  2.2461e-02,  ..., -2.6550e-03,\n",
       "          3.1738e-02, -1.0681e-03],\n",
       "        [ 1.9775e-02,  2.0020e-02,  2.8687e-02,  ..., -3.5248e-03,\n",
       "          3.1433e-03, -7.6294e-03],\n",
       "        ...,\n",
       "        [-3.0975e-03,  2.1057e-03,  4.8828e-03,  ..., -2.0905e-03,\n",
       "         -1.2207e-03, -2.8992e-03],\n",
       "        [-3.0975e-03,  2.1057e-03,  4.8828e-03,  ..., -2.0905e-03,\n",
       "         -1.2207e-03, -2.8992e-03],\n",
       "        [-3.0975e-03,  2.1057e-03,  4.8828e-03,  ..., -2.0905e-03,\n",
       "         -1.2207e-03, -2.8992e-03]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#开始创建embeding_layer\n",
    "embedding_layer = torch.nn.Embedding(config['vocab_size'], config[\"hidden_size\"])\n",
    "embedding_layer.weight.data.copy_(state_dict1[\"model.embed_tokens.weight\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_acc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
