{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama3.2 from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此notebook的目的是学习，attention layer，mlp layer等的基础搭建，然后可以从safetensor中load权重。本文暂时不涉及kv cache等优化与推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "首先从huggingface下载Llama-3.2-3B-Instruct，具体步骤是：\n",
    "1. Pip安装huggingface-cli\n",
    "`pip install -U huggingface_hub`\n",
    "2. 执行`huggingface-cli login`，输入token(token来自于自己的注册)\n",
    "3. 然后用transformer接口下载模型，默认路径：~/.cache\n",
    "\n",
    "[参考1:transformer源码](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)\n",
    "\n",
    "[参考2:简略版llama3 from scratch](https://github.com/naklecha/llama3-from-scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/HOME/scz0101/.conda/envs/model_acc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0\n"
     ]
    }
   ],
   "source": [
    "# 读取并查看模型参数\n",
    "with open(\"/HOME/scz0101/run/model_acceleration/1_models/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "# print(config)\n",
    "print(config['rope_scaling']['factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取模型权重\n",
    "#from safetensors import safe_open\n",
    "weights_root = \"/HOME/scz0101/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95\"\n",
    "# 文件路径\n",
    "file1 = os.path.join(weights_root, \"model-00001-of-00002.safetensors\")\n",
    "file2 = os.path.join(weights_root, \"model-00002-of-00002.safetensors\")\n",
    "\n",
    "# # 加载第一个文件\n",
    "# with safe_open(file1, framework=\"pt\", device=\"cpu\") as f:\n",
    "#     state_dict1 = {key: f.get_tensor(key) for key in f.keys()}\n",
    "# # 查看key和size\n",
    "# print(json.dumps(list(state_dict1.keys()), indent=4)) ## layer0~20\n",
    "# print(\"model.embed_tokens.weight\", state_dict1[\"model.embed_tokens.weight\"].shape) # vob_size*hidden_size\n",
    "# print(\"model.layers.0.input_layernorm.weight\", state_dict1[\"model.layers.0.input_layernorm.weight\"].shape) #hidden_size\n",
    "# print(\"model.layers.0.mlp.down_proj.weight\", state_dict1[\"model.layers.0.mlp.down_proj.weight\"].shape) #hidden_size * intermediate_size\n",
    "# print(\"model.layers.0.mlp.gate_proj.weight\", state_dict1[\"model.layers.0.mlp.gate_proj.weight\"].shape) # intermediate_size * hidden_size \n",
    "# print(\"model.layers.0.mlp.up_proj.weight\", state_dict1[\"model.layers.0.mlp.up_proj.weight\"].shape) #intermediate_size * hidden_size \n",
    "# print(\"model.layers.0.post_attention_layernorm.weight\",state_dict1[\"model.layers.0.post_attention_layernorm.weight\"].shape) #hidden_size\n",
    "# print(\"model.layers.0.self_attn.k_proj.weight\",state_dict1[\"model.layers.0.self_attn.k_proj.weight\"].shape) #(head_dim*num_key_value_heads)*hidden_size\n",
    "# print(\"model.layers.0.self_attn.o_proj.weight\",state_dict1[\"model.layers.0.self_attn.o_proj.weight\"].shape) #(head_dim*num_attention_heads)*hidden_size\n",
    "# print(\"model.layers.0.self_attn.q_proj.weight\",state_dict1[\"model.layers.0.self_attn.q_proj.weight\"].shape) #(head_dim*num_attention_heads)*hidden_size\n",
    "# print(\"model.layers.0.self_attn.v_proj.weight\",state_dict1[\"model.layers.0.self_attn.v_proj.weight\"].shape) #(head_dim*num_key_value_heads)*hidden_size\n",
    "\n",
    "# # # 加载第二个文件(为了节省时间，可以先skip)\n",
    "# with safe_open(file2, framework=\"pt\", device=\"cpu\") as f:\n",
    "#     state_dict2 = {key: f.get_tensor(key) for key in f.keys()}\n",
    "\n",
    "# # 合并两个状态字典\n",
    "# state_dict = {**state_dict1, **state_dict2}\n",
    "\n",
    "# # 查看模型的权重key\n",
    "# print(json.dumps(list(state_dict2.keys()), indent=4)) ## layer21~27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建rms_norm层，tensor2维，对每一行做归一化\n",
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备位置编码（RoPE）\n",
    "import math\n",
    "\n",
    "def get_inv_freq_llama3(device):\n",
    "    dim = config[\"head_dim\"]\n",
    "    rope_theta = config[\"rope_theta\"]\n",
    "    inv_freq = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n",
    "    # print(inv_freq)\n",
    "    attention_factor = 1.0\n",
    "    # 对于llama3的特殊处理\n",
    "    factor = config[\"rope_scaling\"][\"factor\"]  # `8` in the original implementation\n",
    "    low_freq_factor = config[\"rope_scaling\"][\"low_freq_factor\"]  # `1` in the original implementation\n",
    "    high_freq_factor = config[\"rope_scaling\"][\"high_freq_factor\"]  # `4` in the original implementation\n",
    "    old_context_len = config[\"rope_scaling\"][\"original_max_position_embeddings\"]  # `8192` in the original implementation\n",
    "\n",
    "    low_freq_wavelen = old_context_len / low_freq_factor\n",
    "    high_freq_wavelen = old_context_len / high_freq_factor\n",
    "\n",
    "\n",
    "\n",
    "    wavelen = 2 * math.pi / inv_freq\n",
    "\n",
    "    # print(low_freq_wavelen, high_freq_wavelen, wavelen)\n",
    "    # wavelen < high_freq_wavelen: do nothing\n",
    "    # wavelen > low_freq_wavelen: divide by factor\n",
    "    inv_freq_llama = torch.where(wavelen > low_freq_wavelen, inv_freq / factor, inv_freq)\n",
    "    # otherwise: interpolate between the two, using a smooth factor\n",
    "    smooth_factor = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n",
    "    smoothed_inv_freq = (1 - smooth_factor) * inv_freq_llama / factor + smooth_factor * inv_freq_llama\n",
    "    is_medium_freq = ~(wavelen < high_freq_wavelen) * ~(wavelen > low_freq_wavelen)\n",
    "    inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
    "    return inv_freq_llama, attention_factor\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "    参考：https://blog.csdn.net/UCB001/article/details/139511775\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.intermediate_size = config[\"intermediate_size\"]\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config[\"mlp_bias\"])\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config[\"mlp_bias\"])\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config[\"mlp_bias\"])\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "import torch\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def sdpa_attention_forward(\n",
    "    module: torch.nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    dropout: float = 0.0,\n",
    "    scaling: Optional[float] = None,\n",
    "    is_causal: Optional[bool] = None\n",
    ") -> Tuple[torch.Tensor, None]:\n",
    "    if hasattr(module, \"num_key_value_groups\"):\n",
    "        key = repeat_kv(key, module.num_key_value_groups)\n",
    "        value = repeat_kv(value, module.num_key_value_groups)\n",
    "    # key shape 1,24,5,128\n",
    "    # value shape 1,24,5,128\n",
    "\n",
    "    attention_mask = attention_mask.unsqueeze(0).unsqueeze(0)  # 增加 num_heads 和 seq_len 维度\n",
    "    attention_mask = attention_mask.expand(key.shape[0], key.shape[1], key.shape[2], key.shape[2]) # 1*24*5*5\n",
    "    # print(key.shape[-2])\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n",
    "\n",
    "    # SDPA with memory-efficient backend is bugged with non-contiguous inputs and custom attn_mask for some torch versions\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
    "    query = query.contiguous()\n",
    "    key = key.contiguous()\n",
    "    value = value.contiguous()\n",
    "\n",
    "    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n",
    "    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n",
    "    if is_causal is None:\n",
    "        is_causal = causal_mask is None and query.shape[2] > 1\n",
    "\n",
    "    # Shapes (e.g. query.shape[2]) are tensors during jit tracing, resulting in `is_causal` being a tensor.\n",
    "    # We convert it to a bool for the SDPA kernel that only accepts bools.\n",
    "    if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n",
    "        is_causal = is_causal.item()\n",
    "\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=dropout,\n",
    "        scale=scaling,\n",
    "        is_causal=is_causal,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, None\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = config[\"head_dim\"]\n",
    "        self.num_key_value_groups = config[\"num_attention_heads\"] // config[\"num_key_value_heads\"]\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.attention_dropout = config['attention_dropout']\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config['hidden_size'], config['num_attention_heads'] * self.head_dim, bias=config['attention_bias']\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config['hidden_size'], config['num_key_value_heads'] * self.head_dim, bias=config['attention_bias']\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config['hidden_size'], config['num_key_value_heads'] * self.head_dim, bias=config['attention_bias']\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config['num_attention_heads'] * self.head_dim, config['hidden_size'], bias=config['attention_bias']\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        attn_output, attn_weights = sdpa_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "\n",
    "        self.self_attn = LlamaAttention(layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP()\n",
    "        self.input_layernorm = LlamaRMSNorm(config['hidden_size'], eps=config['rms_norm_eps'])\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config['hidden_size'], eps=config['rms_norm_eps'])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None # necessary, but kept here for BC\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_embeddings=position_embeddings\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, device=None):\n",
    "        super().__init__()\n",
    "        # BC: \"rope_type\" was originally \"type\"\n",
    "\n",
    "        self.rope_type = config['rope_scaling']['rope_type']\n",
    "        self.max_seq_len_cached = config['max_position_embeddings']\n",
    "        self.original_max_seq_len = config['max_position_embeddings']\n",
    "\n",
    "        self.config = config\n",
    "        self.rope_init_fn = get_inv_freq_llama3\n",
    "\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        # Core RoPE block\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False): #enabled=False表示在此上下文中禁用混合精度，所有计算将使用默认的浮点精度（通常是FP32）\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "\n",
    "        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n",
    "        cos = cos * self.attention_scaling\n",
    "        sin = sin * self.attention_scaling\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.padding_idx = config['pad_token_id']\n",
    "        self.vocab_size = config['vocab_size']\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(self.vocab_size, config['hidden_size'])\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(layer_idx) for layer_idx in range(config['num_hidden_layers'])]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config['hidden_size'], eps=config['rms_norm_eps'])\n",
    "        self.rotary_emb = LlamaRotaryEmbedding()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None\n",
    "    ):\n",
    "\n",
    "        # if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "        #     raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        inputs_embeds = self.embed_tokens(input_ids) #b,s,hidden_dim\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "        \n",
    "        # 此处没有使用kv cache,因此每次都需要计算整个score矩阵 ,因此需要一个完整的mask\n",
    "        attention_mask = torch.full((position_ids.shape[1], position_ids.shape[1]), float(\"-inf\"), device=inputs_embeds.device).triu_(1)\n",
    "\n",
    "        for decoder_layer in self.layers[: config['num_hidden_layers']]:\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_embeddings=position_embeddings\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel()\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.lm_head = nn.Linear(config['hidden_size'], config['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0\n",
    "    ):\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(logits, temperature: float = 1.0):\n",
    "    \"\"\"\n",
    "    Samples a token from the logits using temperature scaling.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): The logits tensor for token predictions.\n",
    "        temperature (float, optional): Temperature for scaling logits. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The sampled token.\n",
    "    \"\"\"\n",
    "    logits = logits / max(temperature, 1e-5)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    res = probs.div_(torch.empty_like(probs).exponential_(1)).argmax(dim=-1)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model succ!\n",
      "torch.Size([128256, 3072])\n",
      "torch.Size([128256, 3072])\n",
      "load weights succ!\n",
      "tensor([[3815]])\n",
      " doing\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "model = LlamaForCausalLM()\n",
    "print(\"create model succ!\")\n",
    "\n",
    "# load weghts\n",
    "weights_root = \"/HOME/scz0101/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95\"\n",
    "file1 = os.path.join(weights_root, \"model-00001-of-00002.safetensors\")\n",
    "file2 = os.path.join(weights_root, \"model-00002-of-00002.safetensors\")\n",
    "model_part1 = load_file(file1)\n",
    "model_part2 = load_file(file2)\n",
    "model_state_dict = {**model_part1, **model_part2}\n",
    "\n",
    "print(model.state_dict()[\"lm_head.weight\"].shape)\n",
    "print(model_state_dict[\"model.embed_tokens.weight\"].shape)\n",
    "for weight_name in  model.state_dict().keys():\n",
    "    if weight_name != \"lm_head.weight\":\n",
    "        model.state_dict()[weight_name].copy_(model_state_dict[weight_name])\n",
    "    else:\n",
    "        model.state_dict()[weight_name].copy_(model_state_dict[\"model.embed_tokens.weight\"])\n",
    "\n",
    "print(\"load weights succ!\")\n",
    "\n",
    "# run  model  once\n",
    "# tokenizer先用transfomer自带的 \n",
    "model_name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "input_text = \"how are you? I'm\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "past_seen_tokens = 0\n",
    "temperature = 0\n",
    "# max_new_tokens = 512\n",
    "position_ids = torch.arange(past_seen_tokens,past_seen_tokens+inputs[\"input_ids\"].shape[1]).unsqueeze(0)\n",
    "\n",
    "logits = model.forward(inputs[\"input_ids\"], position_ids, logits_to_keep=1)\n",
    "# logits = model.forward(torch.tensor([[128000, 5269, 527, 499, 30]], dtype=torch.int32), position_ids, logits_to_keep=1)\n",
    "\n",
    "if temperature > 0:\n",
    "    next_token = sample(logits, temperature)\n",
    "else:\n",
    "    next_token = logits.argmax(dim=-1)\n",
    "print(next_token)\n",
    "response = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_acc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
